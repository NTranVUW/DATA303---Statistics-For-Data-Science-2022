---
title: "DATA 303/473 Assignment 2"
author: "Nicholas Tran, 300296259"
date: "Due: 31 March 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(car)
library(mgcv)
library(dplyr)
library(psych)
library(pander)
library(ggplot2)
library(gridExtra)
```

# Q1. (35 marks)

**a. (3 marks)**

```{r}
hybrid <- read.csv("hybrid_reg.csv")
hybrid$year <- as.factor(hybrid$year)
hybrid$carclass <- as.factor(hybrid$carclass)
hybrid$carclass_id <- as.factor(hybrid$carclass_id)
hybrid <- hybrid %>% mutate(msrp.1000 = msrp/1000, 
                            yr_group = case_when(
                              year %in% 1997:2004 ~ "1997-2004",
                              year %in% 2005:2008 ~ "2005-2008",
                              year %in% 2009:2011 ~ "2009-2011",
                              year %in% 2012:2013 ~ "2012-2013"
                            ))
hybrid$yr_group <- as.factor(hybrid$yr_group)
head(hybrid)
addmargins(table(hybrid$yr_group))
```

**b. (3 marks)**

```{r}
a <- ggplot(data=hybrid, aes(x=yr_group, y=msrp.1000))+geom_boxplot()+
  labs(x="yr_group", y="msrp.1000")

b <- ggplot(data=hybrid, aes(x=accelrate, y=msrp.1000))+geom_point()+
  geom_smooth(method='loess')+labs(x="accelrate", y="msrp.1000")

c <- ggplot(data=hybrid, aes(x=mpg, y=msrp.1000))+geom_point()+
  geom_smooth(method='loess')+labs(x="mpg", y="msrp.1000")

d <- ggplot(data=hybrid, aes(x=mpgmpge, y=msrp.1000))+geom_point()+
  geom_smooth(method='loess')+labs(x="mpgmpge", y="msrp.1000")

e <- ggplot(data=hybrid, aes(x=carclass, y=msrp.1000))+geom_boxplot()+
  labs(x="carclass", y="msrp.1000")

grid.arrange(a, b, c, d, e, nrow=2)
```

From the plots above there's evidence that the relationships between `msrp.1000` + `mpg` and `msrp.1000` + `mpgmpge` are non-linear.

**c. (3 marks)**

```{r}
hybrid %>% 
  select(where(is.numeric)) %>% 
  pairs.panels(method = "spearman", density = TRUE, ellipses = FALSE)
```

There's very strong evidence of multicollinearity between `mpg` and `mpgmpge`, the correlation coefficient between them is 0.94 which is incredibly high. This makes sense as  
`mpgmpge` is the max of `mge` and `mpge`. There's no strong evidence of multicollinearity between the other predictors.

**d. (4 marks)**

```{r}
fit1 <- lm(msrp.1000~yr_group+accelrate+mpg+mpgmpge+carclass, data=hybrid)
pander(summary(fit1))
pander(vif(fit1), digits=2, caption="VIF values")
```

Using the threshold given by: $VIF_{model}=\frac{1}{1-R^2_{model}}=\frac{1}{1-0.6417}=2.79$. We see that the $GVIF^{(1/2*DF)}$ all predictors are less than $VIF_{model}=2.79$ which means there's no evidence of severe multicollinearity. It's surprising as `mpg` was identified possible multicollinearity from the pairwise plots.   

**e. (3 marks)**

```{r}
fit.gam <- gam(msrp.1000~yr_group+s(accelrate)+s(mpg)+s(mpgmpge)+carclass, 
               data=hybrid, method="REML")
r_squared <- summary(fit.gam)$dev.expl
adj_r_squared <- summary(fit.gam)$r.sq
RSE <- summary(fit.gam)$scale

titles <- c("R-squared", "Adj. R-squared", "RSE")
vals <- c(r_squared, adj_r_squared, RSE)

tabl <- data.frame(titles, vals)
pander(tabl, digits=5, caption="GAM")
```

**f. (3 marks)**

```{r}
pander(summary(fit.gam)$s.table, digits=3)
```

From the result of the GAM we see that the relationship between `mpg` and `mrsp.1000` is significantly non-linear (high edf [1 is linear, >2 is non-linear] and low $p$-value). The relationship between `accelrate` and `mrsp.1000` is also significantly non-linear. The relationship between `mpgmpge` and `mrsp.1000` however appears to be insignificant.  

**g. (4 marks)**

```{r}
par(mfrow=c(2,2))
gam.check(fit.gam, k.rep=1000)
```

**Convergence:** The report reports full convergence after 5 iterations which indicates that an optimal solution has been found. If no convergence then the results are unreliable.

**Basis functions:** the $p$-values for `mpg` and `mpgmpge` are really small indicating that the residuals are not randomly distributed so there might not be enough basis functions. The k-index values for both these predictors are less than 1 further providing evidence that more basis functions might be needed, however the edf is not $edf\approx k'$, it's not near the max amount of basis functions so more basis functions may not be needed.

**Q-Q Plot:** There's major deviations from the line which suggest non-normality of errors is present.

**Residuals vs. Fitted:** There's a fanning, as y increases the variance of the residuals increases so there's non-constant variance in the errors.

**Histogram of residuals:** The histogram of residuals is normally distributed.

**Response vs. Fitted:** Not a perfect fit but roughly linear.

**h. (4 marks)**

```{r}
fit.gam2 <- gam(msrp.1000~yr_group+s(accelrate)+s(mpgmpge)+carclass, 
               data=hybrid, method="REML")

fit.gam3 <- gam(msrp.1000~yr_group+s(accelrate)+s(mpg)+carclass, 
               data=hybrid, method="REML")

fit.gam4 <- gam(msrp.1000~yr_group+s(accelrate)+carclass, 
               data=hybrid, method="REML")

aic.gam <- AIC(fit.gam)
aic.gam.2 <- AIC(fit.gam2)
aic.gam.3 <- AIC(fit.gam3)
aic.gam.4 <- AIC(fit.gam4)

modname <- c("All predictors", "-mpg", "-mpgmpge", "-mpg, -mpgmpge")
aicval <- c(aic.gam, aic.gam.2, aic.gam.3, aic.gam.4)

mod.compare <- data.frame(modname, aicval)
names(mod.compare) <- c("Model", "AIC")
pander(mod.compare, digits=3, align='c')
```

**i. (3 marks)**

The model with the lowest AIC is the model with all predictors however the difference between the AIC of the model with all predictors and the model without `mpg` and the one without `mpgmpge` are both less than 2.5 so we apply the rule of parsimony and choose the simpler model, either the model without `mpg` or the one without `mpgmpge` but not excluding both. 
This points to multicollinearity where both predictors are important but only one is neccesary.  

**j. (2 marks)**

It's surprising since the resulting $GVIFs^{(1/2*DF)}$ didn't give any evidence of severe multicollinearity. However from the AIC model selection we see that both `mpg` and `mpgmpge` together doesn't have a great effect on the response however removing both of them increases the AIC greatly which indicate at least one of them is an important predictor for the response.  

**k. (4 marks)**

```{r}
bic.gam <- BIC(fit.gam)
bic.gam.2 <- BIC(fit.gam2)
bic.gam.3 <- BIC(fit.gam3)
bic.gam.4 <- BIC(fit.gam4)

modname <- c("All predictors", "-mpg", "-mpgmpge", "-mpg, -mpgmpge")
bicval <- c(bic.gam, bic.gam.2, bic.gam.3, bic.gam.4)

mod.compare <- data.frame(modname, bicval)
names(mod.compare) <- c("Model", "BIC")
pander(mod.compare, digits=3, align='c')
```

The model with the lowest BIC is the one that excludes `mpg`, the next lowest BIC model is the one that excludes `mpgmpge` however the difference is greater than 2.0 so we pick the model without `mpg`. 

# Q2. (5 marks)

**a. (1 marks)**

$\widehat{Y}=\hat{\beta_0}+\hat{\beta_1}X_1+\hat{\beta_2}X_2+\hat{\beta_3}X_3+\hat{\beta_4}(X_1\times X_2)+\hat{\beta_5}(X_1\times X_3)$

$\widehat{Salary}=\hat{\beta_0}+\hat{\beta_1}GPA+\hat{\beta_2}IQ+\hat{\beta_3}GenderMale+\hat{\beta_4}(GPA\times IQ)+\hat{\beta_5}(GPA\times GenderMale)$

$\widehat{Salary}=5+8GPA+0.2IQ+10GenderMale+0.05(GPA\times IQ)+2(GPA\times GenderMale)$

$\widehat{Salary}=\hat{\beta_0}+\hat{\beta_2}IQ+\hat{\beta_3}GenderMale+(\hat{\beta_1}+\hat{\beta_4}IQ+\hat{\beta_5}GenderMale)\times GPA$

$\widehat{Salary}=5+0.2IQ+10GenderMale+(8+0.05IQ+2GenderMale)\times GPA$

**b. (3 marks)**

Keeping GPA and IQ the same we're left with: $\hat{\beta_3}=10$ and $\hat{\beta_5}=2$, $\hat{\beta_3}=10$ means that males earn mroe than females and $\hat{\beta_5}=2$ means that the interaction between males and GPA means that an increase in both results in an increase of salary so males earn higher than females regardless of GPA, it also means that the higher the GPA the wider the gap between salaries between Males and Females. For IQ $\hat{\beta_2}=0.2$ and $\hat{\beta_4}=0.05$, an increase in IQ results in an increase of salary. 

  i. **False.**
  ii. **False.**  
  iii. **True.**
  iv. **False.**
  
**c. (1 marks)**

**False:** Just because the interaction coefficient is small does not mean there's little evidence of an interaction effect. Coefficient of the interaction term does not equal statistical significance, statistical significance testing is usually done via a hypothesis test.    
